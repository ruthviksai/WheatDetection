{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WheatDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5doiWOgwJRnM"
      },
      "source": [
        "# Wheat Detection:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P4XbM1eA2UO"
      },
      "source": [
        "! pip install albumentations==0.4.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLUskZnFxleI"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import re\n",
        "\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import ast\n",
        "import time\n",
        "\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import SequentialSampler"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkQPUQbFBLtR"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive/')\n",
        "!ls /gdrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ7cvTU6xlf9"
      },
      "source": [
        "INPUT_DATA = \"/gdrive/My Drive/global-wheat-detection/\"\n",
        "TRAIN_DIR = os.path.join(INPUT_DATA, \"train\")\n",
        "TEST_DIR = os.path.join(INPUT_DATA, \"test\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU2BbWlzxljN"
      },
      "source": [
        "df = pd.read_csv(os.path.join(INPUT_DATA, \"train.csv\"))\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DStdLanZBUIP"
      },
      "source": [
        "##Unique Images \n",
        "print(f'Unique Images in train DataFrame: {len(df[\"image_id\"].value_counts())}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AizRL4AtBuc_"
      },
      "source": [
        "# Train Valid Split:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0yhTtn9BUQh"
      },
      "source": [
        "images_id = df[\"image_id\"].unique()\n",
        "total_len = len(images_id)\n",
        "train_ratio = 0.8\n",
        "train_len = int(len(images_id)*train_ratio)\n",
        "\n",
        "train_idxs = random.sample(range(total_len), train_len)\n",
        "valid_idxs = np.delete(np.array(range(total_len)), train_idxs)\n",
        "\n",
        "train_ids = images_id[train_idxs]\n",
        "valid_ids = images_id[valid_idxs]\n",
        "\n",
        "print(f\"Total Images Number: {len(images_id)}\")\n",
        "print(f\"Number of training images: {len(train_ids)}\")\n",
        "print(f\"Number of Valid images: {len(valid_ids)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65whtOaiBUZ-"
      },
      "source": [
        "train_df = df[df[\"image_id\"].isin(train_ids)]\n",
        "valid_df = df[df[\"image_id\"].isin(valid_ids)]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfOisKlvBUej"
      },
      "source": [
        "# Data Transform - Albumentation\n",
        "def get_train_transform():\n",
        "    return A.Compose([\n",
        "        A.Flip(0.5),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "\n",
        "def get_valid_transform():\n",
        "    return A.Compose([\n",
        "        ToTensorV2(p=1.0)\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "898XRhgIBUid"
      },
      "source": [
        "class WheatDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, transform=None):\n",
        "        super().__init__()\n",
        "        self.dataframe = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_ids = dataframe[\"image_id\"].unique()\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        image_id = self.image_ids[idx]\n",
        "        details = self.dataframe[self.dataframe[\"image_id\"]==image_id]\n",
        "        img_path = os.path.join(TRAIN_DIR, image_id)+\".jpg\"\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "        \n",
        "        #Row of Dataframe of a particular index.\n",
        "        boxes = details[['x', 'y', 'w', 'h']].values\n",
        "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
        "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
        "        \n",
        "        #To find area\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        \n",
        "        #COnvert it into tensor dataType\n",
        "        area = torch.as_tensor(area, dtype=torch.float32)\n",
        "        \n",
        "        # there is only one class\n",
        "        labels = torch.ones((details.shape[0],), dtype=torch.int64)\n",
        "        \n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((details.shape[0],), dtype=torch.int64)\n",
        "        \n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = labels\n",
        "        target['image_id'] = torch.tensor(idx)\n",
        "        target['area'] = area\n",
        "        target['iscrowd'] = iscrowd\n",
        "        \n",
        "        if self.transform:\n",
        "            sample = {\n",
        "                'image': image,\n",
        "                'bboxes': target['boxes'],\n",
        "                'labels': labels\n",
        "            }\n",
        "            \n",
        "            sample = self.transform(**sample)\n",
        "            image = sample['image']\n",
        "            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "            target[\"boxes\"] = torch.as_tensor(target[\"boxes\"], dtype=torch.long)\n",
        "        \n",
        "        return image, target\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_ids)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u93KpyQRCsyj"
      },
      "source": [
        "# Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEmrwSv5BUmi"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKfsrwprBUqU",
        "outputId": "789cb50c-5503-467c-b723-0818d09a9760"
      },
      "source": [
        "train_dataset = WheatDataset(train_df, TRAIN_DIR, get_train_transform())\n",
        "valid_dataset = WheatDataset(valid_df, TRAIN_DIR, get_valid_transform())\n",
        "\n",
        "print(f\"Length of train_dataset: {len(train_dataset)}\")\n",
        "print(f\"Length of test_dataset: {len(valid_dataset)}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train_dataset: 2698\n",
            "Length of test_dataset: 675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpPGfBIRC1EQ"
      },
      "source": [
        "##DataLoader\n",
        "indices = torch.randperm(len(train_dataset)).tolist()\n",
        "\n",
        "train_data_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "valid_data_loader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUIUONQcJm3_"
      },
      "source": [
        "# Visualize Training Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWOcYzKQC1GY"
      },
      "source": [
        "#PLot images\n",
        "def plot_images(n_num, random_selection=True):\n",
        "    '''Function to visualize N Number of images'''\n",
        "    if random_selection:\n",
        "        index = random.sample(range(0, len(train_df[\"image_id\"].unique())-1), n_num)\n",
        "    else:\n",
        "        index = range(0, n_num)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    fig_no = 1\n",
        "    \n",
        "    for i in index:\n",
        "        images, targets = train_dataset.__getitem__(i)\n",
        "        sample = np.array(np.transpose(images, (1,2,0)))\n",
        "        boxes = targets[\"boxes\"].numpy().astype(np.int32)\n",
        "    \n",
        "        #Plot figure/image\n",
        "\n",
        "        for box in boxes:\n",
        "            cv2.rectangle(sample,(box[0], box[1]),(box[2], box[3]),(255,223,0), 2)\n",
        "        plt.subplot(n_num/2, n_num/2, fig_no)\n",
        "        plt.imshow(sample)\n",
        "        fig_no+=1"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzpbc7KNC1JA"
      },
      "source": [
        "plot_images(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8C70xGaDY7N"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlgKapNRC1Lt"
      },
      "source": [
        "# load a model; pre-trained on COCO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7VbXLSjC1Op"
      },
      "source": [
        "num_classes = 2  # 1 class (wheat) + background\n",
        "\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GRuMQjBC1RR"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "num_epochs = 8"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp_B4q1i_-h9",
        "outputId": "8d594ef4-027d-4827-f350-6a4139e3b6d0"
      },
      "source": [
        "print('Version', torch.__version__)\n",
        "print('CUDA enabled:', torch.cuda.is_available())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Version 1.7.0+cu101\n",
            "CUDA enabled: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moVa4VruC1T-"
      },
      "source": [
        "total_train_loss = []\n",
        "total_valid_loss = []\n",
        "\n",
        "losses_value = 0\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "    start_time = time.time()\n",
        "    train_loss = []\n",
        "    model.train()\n",
        "    \n",
        "    # -----------Training Loop----------------------------\n",
        "    for images, targets in pbar:\n",
        "        \n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        \n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        losses_value = losses.item()\n",
        "        train_loss.append(losses_value)        \n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    total_train_loss.append(np.mean(train_loss))\n",
        "    \n",
        "    \n",
        "    # ---------------Validation Loop----------------------\n",
        "    with torch.no_grad():\n",
        "        valid_loss = []\n",
        "\n",
        "        for images, targets in valid_data_loader:\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            # If you need validation losses\n",
        "            model.train()\n",
        "            # Calculate validation losses\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            loss_value = losses.item()\n",
        "            valid_loss.append(loss_value)\n",
        "            \n",
        "    total_valid_loss.append(np.mean(valid_loss))\n",
        "    \n",
        "    print(f\"Epoch Completed: {epoch+1}/{num_epochs}, Time: {time.time()-start_time},\\\n",
        "    Train Loss: {epoch_train_loss}, Valid Loss: {epoch_valid_loss}\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL9AylRgJyvm"
      },
      "source": [
        "# Plot Train, Valid losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78EZOWdGC1XB"
      },
      "source": [
        "epochs = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "loss = [total_train_loss, total_valid_loss]\n",
        "labels = [\"Train Loss\", \"Validation Loss\"]\n",
        "\n",
        "for arr, label in zip(loss, labels):\n",
        "    plt.plot(epochs, arr, label=label)\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqOvgov3I8AG"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbYR-7vhHfuR"
      },
      "source": [
        "# Data Transform - Test Albumentation\n",
        "def get_test_transform():\n",
        "    return A.Compose([\n",
        "        ToTensorV2(p=1.0)\n",
        "    ])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgRUtyesHfxW"
      },
      "source": [
        "class WheatTestDatasetTest(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, transform=None):\n",
        "        super().__init__()\n",
        "        self.dataframe = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_ids = dataframe[\"image_id\"].unique()\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        image_id = self.image_ids[idx]\n",
        "        #details = self.dataframe[self.dataframe[\"image_id\"]==image_id]\n",
        "        img_path = os.path.join(TEST_DIR, image_id)+\".jpg\"\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "        \n",
        "        if self.transform:\n",
        "            sample = {\n",
        "                'image': image,\n",
        "            }\n",
        "            \n",
        "            sample = self.transform(**sample)\n",
        "            image = sample['image']\n",
        "        \n",
        "        return image, image_id\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_ids)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMT3HoMTHpIt"
      },
      "source": [
        "# Load Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w54XzKxHfzl",
        "outputId": "4f37c08f-ab40-4764-edf2-6bd5659cb1d7"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "test_dataset = WheatTestDatasetTest(df_test, TEST_DIR, get_test_transform())\n",
        "print(f\"Length of test dataset: {len(test_dataset)}\")\n",
        "\n",
        "test_data_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    drop_last=False,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of test dataset: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkqv1Dy8Hucy"
      },
      "source": [
        "# Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMrHcfU-Ysp0"
      },
      "source": [
        "model.eval()\n",
        "x = model.to(device)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjPIX1mdHf5j"
      },
      "source": [
        "detection_threshold = 0.5\n",
        "output_list = []\n",
        "\n",
        "for images, image_ids in test_data_loader:\n",
        "\n",
        "    images = list(image.to(device) for image in images)\n",
        "    outputs = model(images)\n",
        "\n",
        "    for i, image in enumerate(images):\n",
        "\n",
        "        boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
        "        scores = outputs[i]['scores'].data.cpu().numpy()\n",
        "        \n",
        "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
        "        scores = scores[scores >= detection_threshold]\n",
        "        image_id = image_ids[i]\n",
        "        \n",
        "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
        "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
        "        \n",
        "        output_dict = {\n",
        "            'image_id': image_ids[i],\n",
        "            'boxes': outputs[i]['boxes'].data.cpu().numpy(),\n",
        "            'scores': outputs[i]['scores'].data.cpu().numpy()\n",
        "        }\n",
        "        output_list.append(output_dict)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGzKlhv0ILh0"
      },
      "source": [
        "# Predict Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfbVijXXHgAK"
      },
      "source": [
        "## Plot image prediction\n",
        "\n",
        "def predict_images(n_num, random_selection=True):\n",
        "    '''Plot N Number of Predicted Images'''\n",
        "    if random_selection:\n",
        "        index = random.sample(range(0, len(df_test[\"image_id\"].unique())), n_num)\n",
        "    else:\n",
        "        index = range(0, n_num)\n",
        "        \n",
        "    plt.figure(figsize=(15,15))\n",
        "    fig_no = 1\n",
        "    \n",
        "    for i in index:\n",
        "        images, image_id = test_dataset.__getitem__(i)\n",
        "        sample = images.permute(1,2,0).cpu().numpy()\n",
        "        boxes = output_list[i]['boxes']\n",
        "        scores = output_list[i]['scores']\n",
        "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
        "        #Plot figure/image\n",
        "        for box in boxes:\n",
        "            cv2.rectangle(sample,(box[0], box[1]),(box[2], box[3]),(255,223,0), 2)\n",
        "        plt.subplot(n_num/2, n_num/2, fig_no)\n",
        "        plt.imshow(sample)\n",
        "        fig_no+=1"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47rOT1uvZQXl"
      },
      "source": [
        "def print_image(i):\n",
        "  plt.figure(figsize=(6,6))\n",
        "  images, image_id = test_dataset.__getitem__(i)\n",
        "  sample = images.permute(1,2,0).cpu().numpy()\n",
        "  boxes = output_list[i]['boxes']\n",
        "  scores = output_list[i]['scores']\n",
        "  boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
        "  #Plot figure/image\n",
        "  for box in boxes:\n",
        "      cv2.rectangle(sample,(box[0], box[1]),(box[2], box[3]),(255,223,0), 2)\n",
        "  # plt.subplot(n_num/2, n_num/2, fig_no)\n",
        "  plt.imshow(sample)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm0LgZKXHgCa"
      },
      "source": [
        "predict_images(4, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFII8b5WZGBl"
      },
      "source": [
        "print_image(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFHjBIJLITn0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2dEj7JEITq8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtEkMvYEITvB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsx8BwuUITxl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx4dKXg8IT0C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJrmn4ZLIT3I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}